"""
SQLè´¨é‡è¯„ä¼°æµ‹è¯•è„šæœ¬

ä½¿ç”¨LLMè¯„ä¼°Text2SQLç³»ç»Ÿç”Ÿæˆçš„SQLè´¨é‡ã€‚
"""
import json
import sys
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from agents import Text2SQLOrchestrator
from evaluation.sql_evaluator import (
    SQLEvaluator,
    extract_table_names,
    generate_evaluation_report,
)


# æµ‹è¯•ç”¨ä¾‹ - MySQLæŸ¥è¯¢ï¼ˆç¬¦åˆç”¨æˆ·æ—¥å¸¸æé—®æ–¹å¼ï¼‰
TEST_CASES = [
    # ========== ç®€å•æŸ¥è¯¢ ==========
    # åŸºç¡€æŸ¥è¯¢
    "å¸®æˆ‘æŸ¥ä¸€ä¸‹æ‰€æœ‰å®¢æˆ·çš„åç§°å’Œè”ç³»ç”µè¯",
    "æœ‰å“ªäº›å®¢æˆ·çš„ä½™é¢è¶…è¿‡1000ï¼Ÿ",
    "åˆ—å‡ºæ‰€æœ‰çŠ¶æ€æ­£å¸¸çš„å®¢æˆ·",
    "æŸ¥ä¸€ä¸‹æœ€è¿‘ä¸€å‘¨æ³¨å†Œçš„æ–°å®¢æˆ·",
    
    # è®¾å¤‡æŸ¥è¯¢
    "å½“å‰æœ‰å¤šå°‘å°å·²æ¿€æ´»è®¾å¤‡ï¼Ÿ",
    "å“ªäº›è®¾å¤‡å¤„äºæœªæ¿€æ´»çŠ¶æ€ï¼Ÿ",
    "åˆ—å‡ºæ‰€æœ‰åœ¨åŒ—äº¬çš„è®¾å¤‡",
    "æŸ¥ä¸€ä¸‹ç‰ˆæœ¬å·æ˜¯2.0å¼€å¤´çš„è®¾å¤‡",
    
    # ç»Ÿè®¡æŸ¥è¯¢
    "ç»Ÿè®¡å„ä¸ªå›½å®¶çš„å®¢æˆ·æ•°é‡",
    "æ¯ç§å®¢æˆ·ç±»å‹æœ‰å¤šå°‘ä¸ªå®¢æˆ·ï¼Ÿ",
    
    # ========== è”è¡¨æŸ¥è¯¢ ==========
    # å®¢æˆ·-è®¾å¤‡å…³è”
    "å¸®æˆ‘æŸ¥ä¸€ä¸‹æ¯ä¸ªå®¢æˆ·æœ‰å¤šå°‘å°è®¾å¤‡",
    "å“ªäº›å®¢æˆ·æ²¡æœ‰ä»»ä½•è®¾å¤‡ï¼Ÿ",
    "æ‰¾å‡ºè®¾å¤‡æ•°é‡è¶…è¿‡10å°çš„å®¢æˆ·",
    "ç»Ÿè®¡æ¯ä¸ªå®¢æˆ·çš„å·²æ¿€æ´»è®¾å¤‡å’Œæœªæ¿€æ´»è®¾å¤‡æ•°é‡",
    
    # å®¢æˆ·-è´¦æˆ·å…³è”
    "æŸ¥ä¸€ä¸‹æ¯ä¸ªå…¬å¸æœ‰å¤šå°‘ä¸ªç”¨æˆ·è´¦å·",
    "åˆ—å‡ºæ²¡æœ‰è´¦å·çš„å®¢æˆ·",
    
    # å¤šè¡¨å¤æ‚æŸ¥è¯¢
    "æŸ¥çœ‹æµ·åº•æè¿™ä¸ªå®¢æˆ·çš„æ‰€æœ‰è®¾å¤‡ä¿¡æ¯",
    "å“ªäº›å®¢æˆ·æœ‰åœ¨çº¿è®¾å¤‡ä½†ä½™é¢ä¸º0ï¼Ÿ",
    "åˆ—å‡ºå„ä¸ªå®¢æˆ·çš„è”ç³»äººä¿¡æ¯åŠå…¶è®¾å¤‡æ•°é‡",
]


def run_evaluation(
    questions: list[str] = None,
    output_file: str = "tests/evaluation_report.md",
    verbose: bool = False,
):
    """
    è¿è¡ŒSQLè´¨é‡è¯„ä¼°ã€‚
    
    Args:
        questions: è¦è¯„ä¼°çš„é—®é¢˜åˆ—è¡¨
        output_file: æŠ¥å‘Šè¾“å‡ºè·¯å¾„
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    """
    questions = questions or TEST_CASES
    
    print(f"\n{'='*60}")
    print(f"SQLè´¨é‡è¯„ä¼° - å…± {len(questions)} ä¸ªé—®é¢˜")
    print(f"{'='*60}\n")
    
    # åˆå§‹åŒ–ç»„ä»¶
    orchestrator = Text2SQLOrchestrator(use_rag=True, rag_top_k=3)
    evaluator = SQLEvaluator()
    
    # æ”¶é›†ç”Ÿæˆç»“æœ
    generated_cases = []
    
    print("ğŸ“ é˜¶æ®µ1: ç”ŸæˆSQL\n")
    for i, question in enumerate(questions, 1):
        print(f"  [{i}/{len(questions)}] {question[:50]}...")
        
        try:
            result = orchestrator.run(question, verbose=False)
            
            if result.get("error"):
                print(f"    âŒ ç”Ÿæˆå¤±è´¥: {result['error'][:50]}")
                continue
            
            steps = result.get("steps_results", [])
            if steps:
                sql = steps[0].get("query", "")
                if sql:
                    generated_cases.append({
                        "question": question,
                        "sql": sql,
                    })
                    print(f"    âœ… SQLç”ŸæˆæˆåŠŸ")
                else:
                    print(f"    âŒ SQLä¸ºç©º")
            else:
                print(f"    âŒ æ— æŸ¥è¯¢æ­¥éª¤")
                
        except Exception as e:
            print(f"    âŒ å¼‚å¸¸: {str(e)[:50]}")
    
    if not generated_cases:
        print("\nâš ï¸ æ²¡æœ‰æˆåŠŸç”Ÿæˆçš„SQLï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
        return
    
    # è¯„ä¼°SQLè´¨é‡
    print(f"\nğŸ“Š é˜¶æ®µ2: è¯„ä¼°SQLè´¨é‡ ({len(generated_cases)} ä¸ª)\n")
    eval_results = evaluator.evaluate_batch(generated_cases)
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“„ é˜¶æ®µ3: ç”ŸæˆæŠ¥å‘Š\n")
    report = generate_evaluation_report(eval_results)
    
    # ä¿å­˜æŠ¥å‘Š
    output_path = Path(output_file)
    output_path.write_text(report, encoding="utf-8")
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    # ä¿å­˜JSONç»“æœ
    json_path = output_path.with_suffix(".json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(eval_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSONç»“æœå·²ä¿å­˜åˆ°: {json_path}")
    
    # æ‰“å°æ‘˜è¦
    successful = [r for r in eval_results if r["evaluation"]]
    if successful:
        avg_score = sum(r["evaluation"]["overall_score"] for r in successful) / len(successful)
        correct = sum(1 for r in successful if r["evaluation"]["is_correct"])
        
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°å®Œæˆ")
        print(f"  ç»¼åˆå¹³å‡åˆ†: {avg_score:.1f}/10")
        print(f"  æ­£ç¡®ç‡: {correct}/{len(successful)} ({correct/len(successful)*100:.1f}%)")
        print(f"{'='*60}\n")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="SQLè´¨é‡è¯„ä¼°")
    parser.add_argument(
        "-q", "--questions",
        nargs="+",
        help="è¦è¯„ä¼°çš„é—®é¢˜"
    )
    parser.add_argument(
        "-o", "--output",
        default="tests/evaluation_report.md",
        help="æŠ¥å‘Šè¾“å‡ºè·¯å¾„"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º"
    )
    
    args = parser.parse_args()
    
    run_evaluation(
        questions=args.questions,
        output_file=args.output,
        verbose=args.verbose,
    )


if __name__ == "__main__":
    main()

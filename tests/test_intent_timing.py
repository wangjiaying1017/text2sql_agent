"""
æ„å›¾è¯†åˆ«è€—æ—¶æµ‹è¯•è„šæœ¬

æµ‹é‡å„é˜¶æ®µçš„è€—æ—¶åˆ†å¸ƒï¼š
1. Prompt æ„å»ºæ—¶é—´
2. LLM API è°ƒç”¨æ—¶é—´
3. å“åº”è§£ææ—¶é—´
"""
import time
import sys
sys.path.insert(0, 'd:/code_project/text2sql_agent')

from langchain_core.prompts import ChatPromptTemplate
from llm.client import get_qwen_model
from intent.prompts import INTENT_RECOGNITION_SYSTEM_PROMPT, INTENT_RECOGNITION_USER_PROMPT
from intent.recognizer import QueryPlan


def measure_intent_recognition(question: str, context: str = ""):
    """æµ‹é‡æ„å›¾è¯†åˆ«å„é˜¶æ®µè€—æ—¶"""
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é—®é¢˜: {question}")
    print(f"{'='*60}\n")
    
    timings = {}
    
    # 1. æ¨¡å‹åˆå§‹åŒ–
    print("[1/5] åˆå§‹åŒ–æ¨¡å‹...")
    t0 = time.time()
    base_llm = get_qwen_model(temperature=0)
    timings['model_init'] = time.time() - t0
    print(f"   âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {timings['model_init']:.3f}s")
    
    # 2. ç»‘å®šç»“æ„åŒ–è¾“å‡º
    print("[2/5] ç»‘å®šç»“æ„åŒ–è¾“å‡º...")
    t0 = time.time()
    llm = base_llm.with_structured_output(QueryPlan)
    timings['structured_output_binding'] = time.time() - t0
    print(f"   âœ“ ç»“æ„åŒ–è¾“å‡ºç»‘å®šå®Œæˆ: {timings['structured_output_binding']:.3f}s")
    
    # 3. æ„å»º Prompt
    print("[3/5] æ„å»º Prompt...")
    t0 = time.time()
    prompt = ChatPromptTemplate.from_messages([
        ("system", INTENT_RECOGNITION_SYSTEM_PROMPT),
        ("human", INTENT_RECOGNITION_USER_PROMPT),
    ])
    
    # æ ¼å¼åŒ– prompt
    formatted_prompt = prompt.format_messages(
        question=question,
        context=context if context else "æ— å†å²ä¸Šä¸‹æ–‡"
    )
    timings['prompt_construction'] = time.time() - t0
    
    # è®¡ç®— prompt é•¿åº¦
    total_chars = sum(len(msg.content) for msg in formatted_prompt)
    print(f"   âœ“ Prompt æ„å»ºå®Œæˆ: {timings['prompt_construction']:.3f}s")
    print(f"   ğŸ“ Prompt æ€»å­—ç¬¦æ•°: {total_chars:,}")
    
    # 4. LLM API è°ƒç”¨
    print("[4/5] è°ƒç”¨ LLM API...")
    t0 = time.time()
    chain = prompt | llm
    result = chain.invoke({
        "question": question,
        "context": context if context else "æ— å†å²ä¸Šä¸‹æ–‡",
    })
    timings['llm_api_call'] = time.time() - t0
    print(f"   âœ“ LLM API è°ƒç”¨å®Œæˆ: {timings['llm_api_call']:.3f}s")
    
    # 5. åå¤„ç†
    print("[5/5] åå¤„ç†...")
    t0 = time.time()
    for step in result.steps:
        step.database = step.database.lower()
    timings['post_processing'] = time.time() - t0
    print(f"   âœ“ åå¤„ç†å®Œæˆ: {timings['post_processing']:.3f}s")
    
    # æ±‡æ€»
    total_time = sum(timings.values())
    
    print(f"\n{'='*60}")
    print("ğŸ“Š è€—æ—¶åˆ†å¸ƒæ±‡æ€»")
    print(f"{'='*60}")
    print(f"{'é˜¶æ®µ':<30} {'è€—æ—¶':<10} {'å æ¯”':<10}")
    print(f"{'-'*60}")
    
    for stage, duration in timings.items():
        stage_name = {
            'model_init': 'æ¨¡å‹åˆå§‹åŒ–',
            'structured_output_binding': 'ç»“æ„åŒ–è¾“å‡ºç»‘å®š',
            'prompt_construction': 'Prompt æ„å»º',
            'llm_api_call': 'LLM API è°ƒç”¨',
            'post_processing': 'åå¤„ç†'
        }.get(stage, stage)
        percentage = (duration / total_time) * 100
        bar = 'â–ˆ' * int(percentage / 2)
        print(f"{stage_name:<25} {duration:>6.3f}s  {percentage:>5.1f}% {bar}")
    
    print(f"{'-'*60}")
    print(f"{'æ€»è®¡':<25} {total_time:>6.3f}s  100.0%")
    print(f"{'='*60}\n")
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    print("ğŸ“‹ è¯†åˆ«ç»“æœ:")
    print(f"   ç½®ä¿¡åº¦: {result.confidence}")
    print(f"   éœ€è¦æ¾„æ¸…: {result.needs_clarification}")
    print(f"   æ­¥éª¤æ•°é‡: {len(result.steps)}")
    if result.steps:
        for step in result.steps:
            print(f"   - Step {step.step}: [{step.database}] {step.purpose}")
    
    return timings, result


def main():
    """è¿è¡Œæµ‹è¯•"""
    # æµ‹è¯•ç”¨ä¾‹
    test_questions = [
        "æŸ¥è¯¢å®¢æˆ·å¼ ä¸‰çš„è®¾å¤‡åˆ—è¡¨",
        "æŸ¥è¯¢åºåˆ—å·ä¸ºABC123çš„è®¾å¤‡æœ€è¿‘1å°æ—¶çš„æµé‡æ•°æ®",
        "ç»Ÿè®¡æ‰€æœ‰åœ¨çº¿è®¾å¤‡çš„æ•°é‡",
    ]
    
    print("\n" + "="*70)
    print("ğŸ”¬ æ„å›¾è¯†åˆ«è€—æ—¶æµ‹è¯•")
    print("="*70)
    
    all_timings = []
    
    for q in test_questions:
        try:
            timings, _ = measure_intent_recognition(q)
            all_timings.append(timings)
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # è®¡ç®—å¹³å‡è€—æ—¶
    if len(all_timings) > 1:
        print("\n" + "="*70)
        print("ğŸ“Š å¹³å‡è€—æ—¶ç»Ÿè®¡ (åŸºäº {} ä¸ªæµ‹è¯•)".format(len(all_timings)))
        print("="*70)
        
        avg_timings = {}
        for key in all_timings[0].keys():
            avg_timings[key] = sum(t[key] for t in all_timings) / len(all_timings)
        
        total_avg = sum(avg_timings.values())
        for stage, duration in avg_timings.items():
            stage_name = {
                'model_init': 'æ¨¡å‹åˆå§‹åŒ–',
                'structured_output_binding': 'ç»“æ„åŒ–è¾“å‡ºç»‘å®š',
                'prompt_construction': 'Prompt æ„å»º',
                'llm_api_call': 'LLM API è°ƒç”¨',
                'post_processing': 'åå¤„ç†'
            }.get(stage, stage)
            print(f"{stage_name:<25} {duration:>6.3f}s")
        print(f"{'-'*40}")
        print(f"{'å¹³å‡æ€»è€—æ—¶':<25} {total_avg:>6.3f}s")


if __name__ == "__main__":
    main()

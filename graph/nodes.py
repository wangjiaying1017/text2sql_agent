"""
LangGraph èŠ‚ç‚¹å‡½æ•°

å®ç° Text2SQL å·¥ä½œæµçš„å„ä¸ªèŠ‚ç‚¹ï¼Œä½¿ç”¨ Command æ¨¡å¼è¿›è¡ŒçŠ¶æ€æ›´æ–°å’Œæµç¨‹æ§åˆ¶ã€‚
"""
from typing import Any, Optional
import time
import json
import logging
from langgraph.types import Command
from langchain_core.runnables import RunnableConfig

from .state import Text2SQLState

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger("text2sql.nodes")


# ============== æ—¥å¿—è¾…åŠ©å‡½æ•° ==============
def log_node_start(node_name: str, state: Text2SQLState, show_fields: list[str] = None):
    """è®°å½•èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œï¼ˆDEBUG çº§åˆ«ï¼‰ã€‚"""
    logger.info(f"[{node_name}] å¼€å§‹æ‰§è¡Œ")
    
    # DEBUG çº§åˆ«æ‰æ˜¾ç¤ºè¯¦ç»†è¾“å…¥
    if logger.isEnabledFor(logging.DEBUG) and show_fields:
        for field in show_fields:
            value = state.get(field)
            if value is not None:
                str_value = str(value)[:100] + "..." if len(str(value)) > 100 else str(value)
                logger.debug(f"  è¾“å…¥ {field}: {str_value}")






# ============== å•ä¾‹ç¼“å­˜ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–ï¼‰ ==============
_intent_recognizer = None
_mysql_retriever = None
_influxdb_retriever = None
_sql_generator = None


def get_intent_recognizer():
    """è·å– IntentRecognizer å•ä¾‹ã€‚"""
    global _intent_recognizer
    if _intent_recognizer is None:
        from intent import IntentRecognizer
        logger.debug("é¦–æ¬¡åˆå§‹åŒ– IntentRecognizer...")
        _intent_recognizer = IntentRecognizer()
    return _intent_recognizer


def get_hybrid_retriever(database_type: str):
    """è·å– HybridRetriever å•ä¾‹ï¼ˆæŒ‰æ•°æ®åº“ç±»å‹ï¼‰ã€‚"""
    global _mysql_retriever, _influxdb_retriever
    from retrieval import HybridRetriever
    
    if database_type == "mysql":
        if _mysql_retriever is None:
            logger.debug("é¦–æ¬¡åˆå§‹åŒ– MySQL HybridRetriever...")
            _mysql_retriever = HybridRetriever(database_type="mysql")
        return _mysql_retriever
    else:
        if _influxdb_retriever is None:
            logger.debug("é¦–æ¬¡åˆå§‹åŒ– InfluxDB HybridRetriever...")
            _influxdb_retriever = HybridRetriever(database_type="influxdb")
        return _influxdb_retriever


def get_sql_generator():
    """è·å– SQLGenerator å•ä¾‹ã€‚"""
    global _sql_generator
    if _sql_generator is None:
        from agents.sql_generator import SQLGenerator
        logger.debug("é¦–æ¬¡åˆå§‹åŒ– SQLGenerator...")
        _sql_generator = SQLGenerator()
    return _sql_generator


def warmup_all(database_types: list[str] = None) -> dict[str, float]:
    """
    é¢„çƒ­æ‰€æœ‰ç»„ä»¶çš„è¿æ¥å’Œ APIã€‚
    
    åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘é¦–æ¬¡æŸ¥è¯¢çš„å»¶è¿Ÿã€‚
    
    Args:
        database_types: è¦é¢„çƒ­çš„æ•°æ®åº“ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤ ["mysql"]
        
    Returns:
        å„ç»„ä»¶é¢„çƒ­è€—æ—¶çš„å­—å…¸
    """
    import time
    
    if database_types is None:
        database_types = ["mysql"]
    
    logger.info("[WARMUP] å¼€å§‹é¢„çƒ­...")
    
    timings = {}
    total_start = time.time()
    
    # 1. é¢„çƒ­ IntentRecognizerï¼ˆLLM å®¢æˆ·ç«¯ + Qdrantï¼‰
    t0 = time.time()
    intent_recognizer = get_intent_recognizer()
    timings["intent_recognizer_init"] = time.time() - t0
    logger.info(f"  IntentRecognizer åˆå§‹åŒ–: {timings['intent_recognizer_init']:.2f}s")
    
    # 1.1 é¢„çƒ­ IntentRecognizer çš„ Qdrant è¿æ¥ï¼ˆå¹¶è¡Œï¼‰
    intent_warmup = intent_recognizer.warmup()
    timings["intent_qdrant_warmup"] = intent_warmup.get("total", 0)
    
    # 2. é¢„çƒ­ HybridRetrieverï¼ˆQdrant + ES + Embedding APIï¼‰
    for db_type in database_types:
        retriever = get_hybrid_retriever(db_type)
        retriever_timings = retriever.warmup()
        for key, value in retriever_timings.items():
            timings[f"retriever_{key}"] = value
    
    # 3. é¢„çƒ­ SQLGeneratorï¼ˆLLM å®¢æˆ·ç«¯ï¼‰
    t0 = time.time()
    get_sql_generator()
    timings["sql_generator_init"] = time.time() - t0
    logger.info(f"  SQLGenerator åˆå§‹åŒ–: {timings['sql_generator_init']:.2f}s")
    
    total_time = time.time() - total_start
    timings["total"] = total_time
    
    logger.info(f"[WARMUP] é¢„çƒ­å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s")
    
    return timings
# ======================================================


def format_context(
    results: list[dict[str, Any]], 
    max_rows: int = 20,
    max_tokens: int = 2000
) -> str:
    """
    å°†æŸ¥è¯¢ç»“æœå‹ç¼©ä¸ºæ‘˜è¦ä¸Šä¸‹æ–‡ï¼Œé¿å…ä¸Šä¸‹æ–‡æº¢å‡ºã€‚
    
    ç­–ç•¥ï¼š
    1. é™åˆ¶è¡Œæ•°ï¼šæœ€å¤šä¿ç•™ max_rows æ¡å…³é”®è®°å½•
    2. é™åˆ¶ Tokenï¼šä¼°ç®— token æ•°ï¼Œè¶…é™æ—¶è¿›ä¸€æ­¥å‹ç¼©
    3. ä»…æå–å…³é”®å­—æ®µï¼šå¦‚ idã€nameã€serial ç­‰å¯ä½œä¸ºåç»­æŸ¥è¯¢æ¡ä»¶çš„å­—æ®µ
    """
    if not results:
        return "ä¸Šä¸€æ­¥æŸ¥è¯¢æ— ç»“æœ"
    
    total_count = len(results)
    
    # ç­–ç•¥1: é™åˆ¶è¡Œæ•°
    if total_count > max_rows:
        results = results[:max_rows]
        truncated = True
    else:
        truncated = False
    
    # ç­–ç•¥2: æå–å…³é”®å­—æ®µï¼ˆç”¨äºåç»­æŸ¥è¯¢çš„ ID/æ ‡è¯†ç¬¦ï¼‰
    key_fields = ["id", "serial", "client_id", "name", "device_id", "node_id"]
    summaries = []
    for row in results:
        key_values = {k: v for k, v in row.items() if k in key_fields}
        if key_values:
            summaries.append(str(key_values))
        else:
            # å…œåº•ï¼šå–å‰ 3 ä¸ªå­—æ®µ
            summaries.append(str(dict(list(row.items())[:3])))
    
    context = f"ä¸Šä¸€æ­¥æŸ¥è¯¢è¿”å› {total_count} æ¡è®°å½•"
    if truncated:
        context += f"ï¼ˆä»…æ˜¾ç¤ºå‰ {max_rows} æ¡ï¼‰"
    context += ":\n" + "\n".join(summaries)
    
    # ç­–ç•¥3: Token ä¼°ç®—ä¿æŠ¤
    estimated_tokens = len(context) // 4  # ç²—ç•¥ä¼°ç®—
    if estimated_tokens > max_tokens:
        # è¿›ä¸€æ­¥å‹ç¼©ï¼šåªä¿ç•™ ID åˆ—è¡¨
        ids = [r.get("id") or r.get("serial") for r in results[:10] if r.get("id") or r.get("serial")]
        context = f"ä¸Šä¸€æ­¥æŸ¥è¯¢è¿”å› {total_count} æ¡è®°å½•ï¼Œå…³é”®ID: {ids}"
    
    return context


def intent_node(state: Text2SQLState, config: RunnableConfig) -> Command:
    """æ„å›¾è¯†åˆ«èŠ‚ç‚¹ - ç”ŸæˆæŸ¥è¯¢è®¡åˆ’"""
    log_node_start("intent_node", state, ["question", "messages"])
    total_start = time.time()
    
    # 1. è·å– IntentRecognizer å•ä¾‹
    recognizer = get_intent_recognizer()
    
    try:
        # 2. æ„å»ºä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆç»“æ„åŒ–æå– + æœ€è¿‘é—®é¢˜ï¼‰
        from utils.context_utils import extract_context_from_messages, format_extracted_context
        
        context_parts = []
        messages = state.get("messages", [])
        
        if messages:
            # 2.1 æå–ç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼ˆè®¾å¤‡/å®¢æˆ·ä¿¡æ¯ï¼‰
            extracted = extract_context_from_messages(messages, max_history=3)
            if extracted:
                context_parts.append("å‚è€ƒä¿¡æ¯ï¼š\n" + format_extracted_context(extracted))
            
            # 2.2 ä¿ç•™æœ€è¿‘çš„é—®é¢˜ï¼ˆä¸å« AI å›å¤çš„ JSONï¼‰
            recent_questions = [
                msg.content for msg in messages[-4:]
                if msg.type == "human"
            ]
            if recent_questions:
                context_parts.append("æœ€è¿‘çš„é—®é¢˜ï¼š\n" + "\n".join(f"- {q}" for q in recent_questions))
        
        context = "\n\n".join(context_parts)
        if context:
            logger.debug(f"ä½¿ç”¨ç»“æ„åŒ–ä¼šè¯ä¸Šä¸‹æ–‡")
        
        # 3. æ‰§è¡Œæ„å›¾è¯†åˆ«ï¼ˆå¼€å¯ verbose æŸ¥çœ‹ promptï¼‰
        plan = recognizer.recognize(state["question"], context=context, verbose=True)
        
        total_time = time.time() - total_start
        
        if not plan.steps:
            updates = {"error": "æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„æŸ¥è¯¢è®¡åˆ’", "timing": {"intent": round(total_time, 2)}}
            return Command(update=updates, goto="error_handler")
        
        # è¾“å‡ºç”Ÿæˆçš„æŸ¥è¯¢è®¡åˆ’
        logger.info(f"[intent_node] ç”ŸæˆæŸ¥è¯¢è®¡åˆ’ï¼šå…± {len(plan.steps)} æ­¥")
        for step in plan.steps:
            logger.info(f"  Step {step.step}: [{step.database}] {step.purpose}")
        
        updates = {
            "query_plan": plan.model_dump(),
            "total_steps": len(plan.steps),
            "current_step": 0,
            "retry_count": 0,
            "timing": {"intent": round(total_time, 2)}
        }
        return Command(update=updates)
        
    except Exception as e:
        total_time = time.time() - total_start
        updates = {"error": f"æ„å›¾è¯†åˆ«å¤±è´¥: {str(e)}", "timing": {"intent": round(total_time, 2)}}
        return Command(update=updates, goto="error_handler")


def plan_validator_node(state: Text2SQLState, config: RunnableConfig) -> Command:
    """è®¡åˆ’æ ¡éªŒèŠ‚ç‚¹ - éªŒè¯ç”Ÿæˆçš„è®¡åˆ’æ˜¯å¦åˆç†"""
    log_node_start("plan_validator_node", state, ["query_plan"])
    
    plan = state["query_plan"]
    errors = []
    
    # æ ¡éªŒ1: æ­¥éª¤ä¸èƒ½ä¸ºç©º
    if not plan.get("steps"):
        errors.append("æŸ¥è¯¢è®¡åˆ’æ²¡æœ‰æ‰§è¡Œæ­¥éª¤")
        updates = {"error": f"è®¡åˆ’æ ¡éªŒå¤±è´¥: {'; '.join(errors)}"}
        return Command(update=updates, goto="error_handler")
    
    steps = plan.get("steps", [])
    
    # æ ¡éªŒ2: æ­¥éª¤ç¼–å·è¿ç»­æ€§
    if len(steps) > 1:
        step_ids = [s["step"] for s in steps]
        expected_from_0 = list(range(len(step_ids)))
        expected_from_1 = list(range(1, len(step_ids) + 1))
        if step_ids != expected_from_0 and step_ids != expected_from_1:
            errors.append(f"æ­¥éª¤ç¼–å·ä¸è¿ç»­: {step_ids}")
    
    # æ ¡éªŒ3: æ•°æ®åº“ç±»å‹æœ‰æ•ˆæ€§
    valid_dbs = {"mysql", "influxdb"}
    for step in steps:
        if step.get("database") not in valid_dbs:
            errors.append(f"æ­¥éª¤{step['step']}çš„æ•°æ®åº“ç±»å‹æ— æ•ˆ: {step.get('database')}")
    
    # æ ¡éªŒ4: ä¾èµ–å…³ç³»åˆç†æ€§
    if len(steps) > 1:
        for step in steps:
            depends_on = step.get("depends_on")
            if depends_on is not None:
                if depends_on >= step["step"]:
                    errors.append(f"æ­¥éª¤{step['step']}ä¾èµ–äº†åç»­æ­¥éª¤{depends_on}")
    
    if errors:
        updates = {"error": f"è®¡åˆ’æ ¡éªŒå¤±è´¥: {'; '.join(errors)}"}
        return Command(update=updates, goto="error_handler")
    
    return Command(update={})  # æ ¡éªŒé€šè¿‡ï¼Œç»§ç»­æ‰§è¡Œ


def rag_node(state: Text2SQLState, config: RunnableConfig) -> Command:
    """RAGæ£€ç´¢èŠ‚ç‚¹ - è·å–ç›¸å…³ Schema"""
    log_node_start("rag_node", state, ["question", "current_step", "total_steps", "current_context"])
    total_start = time.time()
    step = state["query_plan"]["steps"][state["current_step"]]
    
    # 1. è·å– HybridRetriever å•ä¾‹
    retriever = get_hybrid_retriever(step["database"])
    
    # 2. æ„å»ºå¢å¼ºæ£€ç´¢æŸ¥è¯¢
    search_query = state["question"]
    if state["current_step"] > 0 or state["total_steps"] > 1:
        search_query = f"{state['question']} {step['purpose']}"
        logger.debug(f"å¢å¼ºæ£€ç´¢æŸ¥è¯¢: {search_query}")
    
    # 3. æ‰§è¡Œ RAG æ£€ç´¢
    schema = retriever.get_ddl_for_query(search_query)
    
    total_time = time.time() - total_start
    
    logger.info(f"ragè€—æ—¶:{total_time}s")
    return Command(update={
        "current_schema": schema, 
        "timing": {f"rag_step{state['current_step']}": round(total_time, 2)},
        "retry_count": 0
    })


def sql_gen_node(state: Text2SQLState, config: RunnableConfig) -> Command:
    """SQLç”ŸæˆèŠ‚ç‚¹ - æ ¹æ® Schema å’Œä¸Šä¸‹æ–‡ç”Ÿæˆ SQL"""
    log_node_start("sql_gen_node", state, ["question", "current_step", "current_context", "retry_count"])
    total_start = time.time()
    step = state["query_plan"]["steps"][state["current_step"]]
    
    # è·å–ä¸Šä¸‹æ–‡ï¼šä¼˜å…ˆä½¿ç”¨ current_contextï¼ˆå½“å‰æ­¥éª¤çš„ç»“æœï¼‰ï¼Œå¦åˆ™ä» messages å†å²æå–
    from utils.context_utils import extract_context_from_messages, format_extracted_context
    
    context = state.get("current_context", "")
    
    # å¦‚æœæ²¡æœ‰å½“å‰ä¸Šä¸‹æ–‡ï¼Œä»å†å²æ¶ˆæ¯ä¸­æå–
    if not context and state.get("messages"):
        extracted = extract_context_from_messages(state["messages"])
        if extracted:
            context = f"å†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼š\n{format_extracted_context(extracted)}"
            logger.debug(f"ä»å†å²æ¶ˆæ¯æå–ä¸Šä¸‹æ–‡: {extracted}")
    
    # è·å– SQLGenerator å•ä¾‹
    generator = get_sql_generator()
    
    # ç”Ÿæˆ SQL
    sql = generator.generate(
        question=state["question"],
        purpose=step["purpose"],
        database_type=step["database"],
        schema=state["current_schema"],
        context=context if context else "æ— "
    )
    
    total_time = time.time() - total_start
    
    # è®°å½•ç”Ÿæˆçš„ SQL è¯­å¥
    logger.info(f"[sql_gen_node] ç”Ÿæˆ SQL:\n{sql}")
    
    retry_count = state.get("retry_count", 0)
    timing_key = f"sql_gen_step{state['current_step']}"
    if retry_count > 0:
        timing_key += f"_retry{retry_count}"
    
    updates = {
        "current_query": sql,
        "timing": {timing_key: round(total_time, 2)}
    }
    
    return Command(update=updates)


def execute_node(state: Text2SQLState, config: RunnableConfig) -> Command:
    """æ‰§è¡ŒæŸ¥è¯¢èŠ‚ç‚¹ - æ‰§è¡Œ SQL å¹¶å†³å®šä¸‹ä¸€æ­¥ï¼ˆæ”¯æŒé‡è¯•ï¼‰"""
    from database import MySQLConnector, InfluxDBConnector
    
    log_node_start("execute_node", state, ["current_query", "current_step", "retry_count"])
    start_time = time.time()
    step = state["query_plan"]["steps"][state["current_step"]]
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 2)
    
    try:
        # æ ¹æ®æ•°æ®åº“ç±»å‹æ‰§è¡ŒæŸ¥è¯¢
        if step["database"] == "mysql":
            with MySQLConnector() as conn:
                result = conn.execute(state["current_query"])
        else:
            with InfluxDBConnector() as conn:
                result = conn.execute(state["current_query"])
        
        elapsed = time.time() - start_time
        step_timing = {f"execute_step{state['current_step']}": round(elapsed, 2)}
        
        new_step = state["current_step"] + 1
        
        if new_step >= state["total_steps"]:
            # æ‰€æœ‰æ­¥éª¤å®Œæˆ
            # åˆ¤æ–­æ˜¯å¦æœ‰ç»“æœ
            has_results = len(result) > 0
            
            updates = {
                "step_results": [{
                    "step_id": state["current_step"],
                    "database": step["database"],
                    "query": state["current_query"],
                    "results": f"<{len(result)} æ¡è®°å½•>",
                    "error": None
                }],
                "timing": step_timing
            }
            
            return Command(
                update={
                    "step_results": [{
                        "step_id": state["current_step"],
                        "database": step["database"],
                        "query": state["current_query"],
                        "results": result,
                        "error": None
                    }],
                    "timing": step_timing
                },
                goto="aggregate"
            )
        else:
            # è¿˜æœ‰åç»­æ­¥éª¤
            # å¦‚æœä¸­é—´æ­¥éª¤è¿”å›ç©ºç»“æœï¼Œæå‰ç»ˆæ­¢
            if not result:
                step_info = step.get("purpose", f"æ­¥éª¤ {state['current_step'] + 1}")
                error_msg = f"ä¸­é—´æ­¥éª¤æ— ç»“æœï¼š{step_info}ã€‚è¯·æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶æ˜¯å¦æ­£ç¡®ã€‚"
                logger.warning(f"[execute_node] ä¸­é—´æ­¥éª¤è¿”å›ç©ºç»“æœï¼Œæå‰ç»ˆæ­¢")
                
                return Command(
                    update={
                        "status": "no_result",
                        "error": error_msg,
                        "step_results": [{
                            "step_id": state["current_step"],
                            "database": step["database"],
                            "query": state["current_query"],
                            "results": [],
                            "error": error_msg
                        }],
                        "final_results": [],
                        "timing": step_timing
                    },
                    goto="error_handler"
                )
            
            context_summary = format_context(result, max_rows=20, max_tokens=2000)
            updates = {
                "step_results": [{
                    "step_id": state["current_step"],
                    "database": step["database"],
                    "results": f"<{len(result)} æ¡è®°å½•>"
                }],
                "current_step": new_step,
                "current_context": context_summary[:100] + "...",
                "timing": step_timing
            }
            
            return Command(
                update={
                    "step_results": [{
                        "step_id": state["current_step"],
                        "database": step["database"],
                        "query": state["current_query"],
                        "results": result,
                        "error": None
                    }],
                    "current_step": new_step,
                    "current_context": context_summary,
                    "timing": step_timing
                },
                goto="rag"
            )
    except Exception as e:
        elapsed = time.time() - start_time
        error_msg = str(e)
        step_timing = {f"execute_step{state['current_step']}_retry{retry_count}": round(elapsed, 2)}
        
        # åˆ¤æ–­æ˜¯å¦å¯é‡è¯•çš„é”™è¯¯
        is_retryable = any(keyword in error_msg.lower() for keyword in [
            "syntax", "error in your sql", "unknown column", "doesn't exist",
            "table", "ambiguous", "parse",
            # InfluxDB ç›¸å…³é”™è¯¯
            "not executed", "error parsing query", "undefined identifier",
            "invalid", "measurement not found"
        ])
        
        if is_retryable and retry_count < max_retries:
            updates = {
                "retry_count": retry_count + 1,
                "error_msg": error_msg[:100],
            }
            
            return Command(
                update={
                    "retry_count": retry_count + 1,
                    "current_context": f"ä¸Šæ¬¡ SQL æ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {error_msg}\nè¯·ä¿®æ­£ SQL è¯­æ³•é—®é¢˜ã€‚",
                    "timing": step_timing
                },
                goto="sql_gen"
            )
        else:
            if retry_count >= max_retries:
                error_msg = f"å·²é‡è¯• {max_retries} æ¬¡ä»å¤±è´¥: {error_msg}"
            updates = {"error": error_msg[:100]}
            
            return Command(
                update={
                    "error": f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {error_msg}",
                    "timing": step_timing
                },
                goto="error_handler"
            )


def aggregate_node(state: Text2SQLState, config: RunnableConfig) -> Command:
    """ç»“æœèšåˆèŠ‚ç‚¹ - æ±‡æ€»æ‰€æœ‰æ­¥éª¤çš„ç»“æœ"""
    log_node_start("aggregate_node", state, ["step_results"])
    
    if state.get("step_results"):
        last_result = state["step_results"][-1]
        final_results = last_result.get("results", [])
    else:
        final_results = []
    
    # ğŸ†• ä½¿ç”¨ LangChain Message æ ¼å¼ä¿å­˜å¯¹è¯å†å²
    from langchain_core.messages import HumanMessage, AIMessage
    import json
    
    new_messages = []
    
    # æ·»åŠ ç”¨æˆ·åŸå§‹é—®é¢˜
    new_messages.append(HumanMessage(content=state["question"]))
    
    # æ„å»ºç²¾ç®€çš„ AI å›å¤ï¼ˆåªä¿ç•™åç»­å¯¹è¯éœ€è¦çš„å…³é”®ä¿¡æ¯ï¼‰
    from utils.context_utils import extract_key_fields
    
    ai_response = {
        "question": state.get("question"),  # åŸå§‹é—®é¢˜ï¼ˆç”¨äºä¸Šä¸‹æ–‡ç†è§£ï¼‰
        "databases_used": list(set(s.get("database") for s in state.get("step_results", []))),
        "result_count": len(final_results),
        "result_sample": extract_key_fields(final_results, max_rows=5)  # ç²¾ç®€çš„å…³é”®å­—æ®µ
    }
    new_messages.append(AIMessage(content=json.dumps(ai_response, ensure_ascii=False, default=str)))
    
    # åˆ¤æ–­æ˜¯å¦æœ‰ç»“æœ
    has_results = len(final_results) > 0
    status = "success" if has_results else "no_result"
    
    updates = {
        "status": status,
        "final_results": f"<{len(final_results)} æ¡ç»“æœ>",
        "messages": new_messages  # è¿½åŠ åˆ°å†å²ï¼ˆä½¿ç”¨ add_messages reducerï¼‰
    }
    
    return Command(update={
        "status": status,
        "final_results": final_results,
        "messages": new_messages
    })


def error_node(state: Text2SQLState, config: RunnableConfig) -> Command:
    """é”™è¯¯å¤„ç†èŠ‚ç‚¹"""
    log_node_start("error_node", state, ["error", "status"])
    
    # ä¿æŒåŸæœ‰çš„ statusï¼ˆå¯èƒ½æ˜¯ no_resultï¼‰ï¼Œå¦åˆ™è®¾ä¸º error
    current_status = state.get("status")
    final_status = current_status if current_status in ["no_result"] else "error"
    
    updates = {
        "status": final_status,
        "final_results": [],
        "error": state.get("error", "æœªçŸ¥é”™è¯¯")
    }
    
    return Command(update=updates)


def human_input_node(state: Text2SQLState, config: RunnableConfig) -> Command:
    """
    äººå·¥è¾“å…¥èŠ‚ç‚¹ - ç­‰å¾…ç”¨æˆ·ä¸‹ä¸€è½®è¾“å…¥çš„å ä½èŠ‚ç‚¹ã€‚
    
    é…åˆ interrupt_before=["human_input"] ä½¿ç”¨ï¼Œå·¥ä½œæµä¼šåœ¨æ­¤èŠ‚ç‚¹å‰æš‚åœã€‚
    MemorySaver ä¼šè‡ªåŠ¨ä¿å­˜å½“å‰ stateï¼ˆåŒ…å« messages å†å²ï¼‰ã€‚
    ç”¨æˆ·ä¸‹æ¬¡ invoke æ—¶ä¼šæ¢å¤çŠ¶æ€å¹¶ç»§ç»­ã€‚
    """
    log_node_start("human_input_node", state, ["messages"])
    
    # è¿™ä¸ªèŠ‚ç‚¹å®é™…ä¸Šä¸ä¼šè¢«æ‰§è¡Œï¼ˆå› ä¸º interrupt_beforeï¼‰
    # ä½†å¦‚æœæ‰§è¡Œäº†ï¼Œå°±è¿”å›ç©ºæ›´æ–°
    
    return Command(update={})


"""
Qdrant Vector Store for MySQL Schema

å°†MySQLè¡¨ç»“æ„ä¿¡æ¯å­˜å‚¨åˆ°Qdrantå‘é‡æ•°æ®åº“ï¼Œç”¨äºè¯­ä¹‰æ£€ç´¢ã€‚
ä½¿ç”¨é˜¿é‡Œäº‘DashScope text-embedding-v4æ¨¡å‹è¿›è¡Œembeddingã€‚
"""
import json
import re
import sys
from typing import Any, Optional
from pathlib import Path

# ç¡®ä¿èƒ½å¤Ÿå¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent))

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from openai import OpenAI

from config import settings
import logging

logger = logging.getLogger("text2sql.qdrant")


# Qdranté›†åˆåç§°
QDRANT_COLLECTION_NAME = "mysql_table_schema"

# DashScope Embeddingæ¨¡å‹
EMBEDDING_MODEL = "text-embedding-v4"
EMBEDDING_DIM = 1536  # text-embedding-v4 æ”¯æŒå¤šç»´åº¦ï¼Œä½¿ç”¨1536ä¿æŒå…¼å®¹

# å¤–é”®æ³¨é‡Šæ¨¡å¼: (å…³è”t_xxx.yyyå­—æ®µ)
FK_COMMENT_PATTERN = re.compile(r'\(å…³è”\s*(t_\w+)\.(\w+)\s*å­—æ®µ?\)')

# åˆ—åæ¨¡å¼: xxx_id (ç”¨äºæ¨æ–­å¤–é”®)
FK_COLUMN_PATTERN = re.compile(r'^(\w+)_id$')


def extract_foreign_keys(columns: list[dict], table_name: str = "") -> list[dict]:
    """
    æå–å¤–é”®å…³ç³»ã€‚
    
    ç­–ç•¥ï¼š
    1. ä¼˜å…ˆä» COMMENT ä¸­æå–ï¼šåŒ¹é… (å…³è”t_xxx.yyyå­—æ®µ) æ¨¡å¼
    2. å¤‡é€‰ï¼šæ ¹æ®åˆ—åæ¨¡å¼æ¨æ–­ï¼šxxx_id â†’ t_xxx.id
    
    Args:
        columns: åˆ—ä¿¡æ¯åˆ—è¡¨
        table_name: å½“å‰è¡¨åï¼ˆç”¨äºæ’é™¤è‡ªå¼•ç”¨ï¼‰
        
    Returns:
        å¤–é”®å…³ç³»åˆ—è¡¨
    """
    relationships = []
    seen_columns = set()  # é¿å…é‡å¤
    
    for col in columns:
        col_name = col.get('name', '')
        comment = col.get('comment', '')
        
        # è·³è¿‡ä¸»é”®
        if col_name == 'id':
            continue
        
        # ç­–ç•¥1: ä» COMMENT ä¸­æå–
        match = FK_COMMENT_PATTERN.search(comment)
        if match:
            target_table = match.group(1)
            target_column = match.group(2)
            clean_comment = FK_COMMENT_PATTERN.sub('', comment).strip()
            
            relationships.append({
                'column': col_name,
                'target_table': target_table,
                'target_column': target_column,
                'comment': clean_comment
            })
            seen_columns.add(col_name)
            continue
        
        # ç­–ç•¥2: æ ¹æ®åˆ—åæ¨¡å¼æ¨æ–­
        match = FK_COLUMN_PATTERN.match(col_name)
        if match and col_name not in seen_columns:
            prefix = match.group(1)  # e.g., 'client' from 'client_id'
            target_table = f"t_{prefix}"
            
            # æ’é™¤è‡ªå¼•ç”¨
            if target_table == table_name:
                continue
            
            relationships.append({
                'column': col_name,
                'target_table': target_table,
                'target_column': 'id',
                'comment': comment
            })
    
    return relationships


def build_structured_description(schema: dict) -> str:
    """
    æ„å»ºç»“æ„åŒ–çš„è¡¨æè¿°æ–‡æœ¬ï¼Œç”¨äº embeddingã€‚
    
    æ ¼å¼ç¤ºä¾‹:
    Table: t_edge
    Business Meaning: è¾¹ç¼˜èŠ‚ç‚¹ä¿¡æ¯è¡¨
    
    Primary Key:
    - id: æ•°æ®åº“ä¸»é”®id
    
    Important Columns:
    - name: è®¾å¤‡åç§°
    - serial: è®¾å¤‡åºåˆ—å·
    ...
    
    Relationships:
    - t_edge.client_id â†’ t_client.id
    
    Join Hints:
    - JOIN t_client ON t_edge.client_id = t_client.id
    
    Args:
        schema: è¡¨ç»“æ„ä¿¡æ¯
        
    Returns:
        ç»“æ„åŒ–æè¿°æ–‡æœ¬
    """
    table_name = schema.get('table_name', '')
    table_comment = schema.get('table_comment', '')
    columns = schema.get('columns', [])
    
    # æå–ä¸»é”®ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªåˆ—æˆ–åä¸º id çš„åˆ—ï¼‰
    primary_key = next((c for c in columns if c['name'] == 'id'), columns[0] if columns else None)
    
    # æå–é‡è¦åˆ—ï¼ˆå‰ 10 ä¸ªéå¤–é”®åˆ—ï¼‰
    important_cols = columns[:10]
    
    # ä¼˜å…ˆä½¿ç”¨ schema ä¸­å·²æœ‰çš„ relationshipsï¼Œå¦åˆ™ä» columns ä¸­æå–
    relationships = schema.get('relationships', [])
    if not relationships:
        relationships = extract_foreign_keys(columns, table_name)
    
    # æ„å»ºæè¿°æ–‡æœ¬
    lines = [
        f"Table: {table_name}",
        f"Business Meaning: {table_comment}",
        "",
        "Primary Key:",
    ]
    
    if primary_key:
        pk_comment = FK_COMMENT_PATTERN.sub('', primary_key.get('comment', '')).strip()
        lines.append(f"- {primary_key['name']}: {pk_comment}")
    else:
        lines.append("- (unknown)")
    
    lines.append("")
    lines.append("Important Columns:")
    
    for col in important_cols:
        # æ¸…ç†æ³¨é‡Šä¸­çš„å¤–é”®ä¿¡æ¯
        clean_comment = FK_COMMENT_PATTERN.sub('', col.get('comment', '')).strip()
        lines.append(f"- {col['name']}: {clean_comment}")
    
    if relationships:
        lines.append("")
        lines.append("Relationships:")
        for rel in relationships:
            lines.append(f"- {table_name}.{rel['column']} â†’ {rel['target_table']}.{rel['target_column']} ({rel['comment']})")
        
        lines.append("")
        lines.append("Join Hints:")
        for rel in relationships:
            lines.append(f"- JOIN {rel['target_table']} ON {table_name}.{rel['column']} = {rel['target_table']}.{rel['target_column']}")
    
    return "\n".join(lines)


class QdrantStore:
    """
    Qdrantå‘é‡å­˜å‚¨ç±»ï¼Œç”¨äºMySQLè¡¨ç»“æ„çš„è¯­ä¹‰æ£€ç´¢ã€‚
    
    ä½¿ç”¨ç±»å˜é‡å…±äº« QdrantClient å’Œ OpenAI å®¢æˆ·ç«¯ï¼Œé¿å…é‡å¤è¿æ¥ã€‚
    """
    
    # å…±äº«çš„å®¢æˆ·ç«¯å®ä¾‹ï¼ˆç±»å˜é‡ï¼‰
    _shared_client: Optional[QdrantClient] = None
    _shared_openai: Optional[OpenAI] = None
    _warmed_up: bool = False
    
    def __init__(
        self,
        host: Optional[str] = None,
        port: Optional[int] = None,
        collection_name: str = QDRANT_COLLECTION_NAME,
        embedding_model: str = EMBEDDING_MODEL,
    ):
        """
        åˆå§‹åŒ–Qdrantè¿æ¥å’ŒDashScopeå®¢æˆ·ç«¯ã€‚
        
        å¤ç”¨å…±äº«çš„è¿æ¥å®ä¾‹ï¼Œåªåœ¨é¦–æ¬¡åˆ›å»ºæ—¶åˆå§‹åŒ–ã€‚
        
        Args:
            host: Qdrantä¸»æœºåœ°å€
            port: Qdrantç«¯å£
            collection_name: é›†åˆåç§°
            embedding_model: Embeddingæ¨¡å‹åç§°
        """
        self.host = host or settings.qdrant_host
        self.port = port or settings.qdrant_port
        self.collection_name = collection_name
        self.embedding_model = embedding_model
        self._embedding_dim = EMBEDDING_DIM
        
        # å¤ç”¨æˆ–åˆ›å»ºå…±äº«çš„ Qdrant å®¢æˆ·ç«¯
        if QdrantStore._shared_client is None:
            logger.debug(f"[Qdrant] Connecting: {self.host}:{self.port}")
            QdrantStore._shared_client = QdrantClient(host=self.host, port=self.port)
        self._client = QdrantStore._shared_client
        
        # å¤ç”¨æˆ–åˆ›å»ºå…±äº«çš„ OpenAI å®¢æˆ·ç«¯
        if QdrantStore._shared_openai is None:
            logger.debug(f"[Embedding] Model: {embedding_model}, dim: {self._embedding_dim}")
            QdrantStore._shared_openai = OpenAI(
                api_key=settings.qwen_api_key,
                base_url=settings.qwen_base_url,
            )
        self._openai = QdrantStore._shared_openai
    
    def warmup(self) -> float:
        """
        é¢„çƒ­ DashScope Embedding API è¿æ¥ã€‚
        
        é€šè¿‡å‘é€ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•è¯·æ±‚æ¥å®Œæˆï¼š
        - SSL/TLS æ¡æ‰‹
        - DNS è§£æ
        - è¿æ¥æ± åˆå§‹åŒ–
        
        å¦‚æœå·²ç»é¢„çƒ­è¿‡ï¼Œç›´æ¥è¿”å› 0ã€‚
        
        Returns:
            é¢„çƒ­è€—æ—¶ï¼ˆç§’ï¼‰
        """
        # å¦‚æœå·²ç»é¢„çƒ­è¿‡ï¼Œè·³è¿‡
        if QdrantStore._warmed_up:
            return 0.0
        
        import time
        start = time.time()
        try:
            # å‘é€ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•è¯·æ±‚
            self._openai.embeddings.create(
                model=self.embedding_model,
                input="warmup",
                dimensions=self._embedding_dim,
            )
            elapsed = time.time() - start
            logger.debug(f"DashScope API warmup: {elapsed:.2f}s")
            QdrantStore._warmed_up = True
            return elapsed
        except Exception as e:
            elapsed = time.time() - start
            logger.warning(f"DashScope API warmup failed: {e} ({elapsed:.2f}s)")
            return elapsed
    
    def create_collection(self, delete_existing: bool = False) -> None:
        """
        åˆ›å»ºQdranté›†åˆã€‚
        
        Args:
            delete_existing: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ
        """
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        collections = self._client.get_collections().collections
        exists = any(c.name == self.collection_name for c in collections)
        
        if exists:
            if delete_existing:
                print(f"[Qdrant] Deleting existing collection: {self.collection_name}")
                self._client.delete_collection(self.collection_name)
            else:
                print(f"[Qdrant] Collection already exists: {self.collection_name}")
                return
        
        # åˆ›å»ºé›†åˆ
        print(f"[Qdrant] Creating collection: {self.collection_name}")
        self._client.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=self._embedding_dim,
                distance=Distance.COSINE,
            ),
        )
    
    def _build_text_for_embedding(self, schema: dict[str, Any]) -> str:
        """
        æ„å»ºç”¨äºEmbeddingçš„æ–‡æœ¬ã€‚
        
        ä½¿ç”¨ç»“æ„åŒ–æè¿°ä»£æ›¿åŸå§‹DDLï¼ŒåŒ…å«ï¼š
        - è¡¨åå’Œä¸šåŠ¡å«ä¹‰
        - ä¸»é”®
        - é‡è¦åˆ—
        - å¤–é”®å…³ç³»å’ŒJoinæç¤º
        
        Args:
            schema: è¡¨ç»“æ„ä¿¡æ¯
            
        Returns:
            ç”¨äºEmbeddingçš„ç»“æ„åŒ–æè¿°æ–‡æœ¬
        """
        return build_structured_description(schema)
    
    def _get_embedding(self, text: str) -> list[float]:
        """
        è°ƒç”¨DashScope APIç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„embeddingã€‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            embeddingå‘é‡
        """
        import time
        
        if not text.strip():
            # ç©ºæ–‡æœ¬è¿”å›é›¶å‘é‡
            return [0.0] * self._embedding_dim
        
        start = time.time()
        response = self._openai.embeddings.create(
            model=self.embedding_model,
            input=text,
            dimensions=self._embedding_dim,  # DashScope æ”¯æŒè‡ªå®šä¹‰ç»´åº¦
        )
        elapsed = time.time() - start
        # Removed verbose timing log
        
        embedding = response.data[0].embedding
        
        return embedding
    
    def _get_embeddings_batch(self, texts: list[str]) -> list[list[float]]:
        """
        æ‰¹é‡ç”Ÿæˆembeddingã€‚
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            embeddingå‘é‡åˆ—è¡¨
        """
        # è¿‡æ»¤ç©ºæ–‡æœ¬å¹¶è®°å½•ç´¢å¼•
        non_empty_indices = []
        non_empty_texts = []
        for i, text in enumerate(texts):
            if text.strip():
                non_empty_indices.append(i)
                non_empty_texts.append(text)
        
        # æ‰¹é‡è°ƒç”¨API
        embeddings = [[0.0] * self._embedding_dim] * len(texts)
        
        if non_empty_texts:
            # DashScope API æ¯æ¬¡æœ€å¤š10ä¸ªè¾“å…¥
            batch_size = 10
            for start in range(0, len(non_empty_texts), batch_size):
                end = min(start + batch_size, len(non_empty_texts))
                batch_texts = non_empty_texts[start:end]
                batch_indices = non_empty_indices[start:end]
                
                print(f"   å¤„ç† {start+1}-{end}/{len(non_empty_texts)} ...")
                response = self._openai.embeddings.create(
                    model=self.embedding_model,
                    input=batch_texts,
                    dimensions=self._embedding_dim,  # DashScope æ”¯æŒè‡ªå®šä¹‰ç»´åº¦
                )
                
                for j, data in enumerate(response.data):
                    original_idx = batch_indices[j]
                    embeddings[original_idx] = data.embedding
        
        return embeddings
    
    def upsert_schema(self, schema: dict[str, Any], point_id: int) -> None:
        """
        æ’å…¥æˆ–æ›´æ–°å•ä¸ªè¡¨çš„schemaã€‚
        
        Args:
            schema: è¡¨ç»“æ„ä¿¡æ¯
            point_id: ç‚¹ID
        """
        # æ„å»ºEmbeddingæ–‡æœ¬
        text = self._build_text_for_embedding(schema)
        
        # ç”ŸæˆEmbedding
        embedding = self._get_embedding(text)
        
        # æ’å…¥åˆ°Qdrant
        self._client.upsert(
            collection_name=self.collection_name,
            points=[
                PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload=schema,
                )
            ],
        )
    
    def batch_upsert(self, schemas: list[dict[str, Any]]) -> int:
        """
        æ‰¹é‡æ’å…¥è¡¨ç»“æ„ã€‚
        
        Args:
            schemas: è¡¨ç»“æ„åˆ—è¡¨
            
        Returns:
            æˆåŠŸæ’å…¥çš„æ•°é‡
        """
        print(f"[Embedding] Generating vectors...")
        
        # ä¸ºæ¯ä¸ª schema æ·»åŠ ç»“æ„åŒ–æè¿°
        texts = []
        for s in schemas:
            structured_desc = build_structured_description(s)
            s['structured_description'] = structured_desc
            texts.append(structured_desc)
        
        # æ‰¹é‡ç”ŸæˆEmbedding
        embeddings = self._get_embeddings_batch(texts)
        
        # æ„å»ºPoints
        points = [
            PointStruct(
                id=i,
                vector=embeddings[i],
                payload=schemas[i],
            )
            for i in range(len(schemas))
        ]
        
        # æ‰¹é‡æ’å…¥
        print(f"[Qdrant] Writing to Qdrant...")
        self._client.upsert(
            collection_name=self.collection_name,
            points=points,
        )
        
        return len(points)
    
    def search(
        self,
        query: str,
        limit: int = 10,
    ) -> list[dict[str, Any]]:
        """
        è¯­ä¹‰æ£€ç´¢è¡¨ç»“æ„ã€‚
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            limit: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            åŒ¹é…çš„è¡¨ç»“æ„åˆ—è¡¨
        """
        import time
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        t0 = time.time()
        query_embedding = self._get_embedding(query)
        t1 = time.time()
        # Removed verbose timing log
        
        # æœç´¢ï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬APIï¼‰
        try:
            # æ–°ç‰ˆAPI: query_points
            results = self._client.query_points(
                collection_name=self.collection_name,
                query=query_embedding,
                limit=limit,
            ).points
        except AttributeError:
            # æ—§ç‰ˆAPI: search
            results = self._client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=limit,
            )
        t2 = time.time()
        # Removed verbose timing log
        
        # æå–ç»“æœ
        return [
            {
                **hit.payload,
                "_score": hit.score,
            }
            for hit in results
        ]
    
    def get_collection_info(self) -> dict[str, Any]:
        """è·å–é›†åˆä¿¡æ¯ã€‚"""
        info = self._client.get_collection(self.collection_name)
        return {
            "name": self.collection_name,
            "points_count": info.points_count,
            "status": str(info.status),
        }


def import_from_json(json_file: str, delete_existing: bool = False) -> None:
    """
    ä»JSONæ–‡ä»¶å¯¼å…¥è¡¨ç»“æ„åˆ°Qdrantã€‚
    
    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        delete_existing: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ
    """
    with open(json_file, "r", encoding="utf-8") as f:
        schemas = json.load(f)
    
    if not isinstance(schemas, list):
        schemas = [schemas]
    
    store = QdrantStore()
    store.create_collection(delete_existing=delete_existing)
    
    print(f"ğŸ“¥ å¯¼å…¥ {len(schemas)} ä¸ªè¡¨ç»“æ„åˆ°Qdrant...")
    count = store.batch_upsert(schemas)
    print(f"âœ… æˆåŠŸå¯¼å…¥ {count} ä¸ªè¡¨ç»“æ„")
    
    # æ˜¾ç¤ºé›†åˆä¿¡æ¯
    info = store.get_collection_info()
    print(f"ğŸ“Š é›†åˆä¿¡æ¯: {info}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å°†MySQLè¡¨ç»“æ„å¯¼å…¥Qdrantå‘é‡æ•°æ®åº“")
    parser.add_argument(
        "-f", "--file",
        default="schema/all_tables.json",
        help="JSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: schema/all_tables.json)"
    )
    parser.add_argument(
        "-d", "--delete",
        action="store_true",
        help="åˆ é™¤å·²å­˜åœ¨çš„é›†åˆåé‡æ–°åˆ›å»º"
    )
    parser.add_argument(
        "-s", "--search",
        help="æœç´¢å…³é”®è¯ï¼ˆæµ‹è¯•ç”¨ï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.search:
        # æœç´¢æ¨¡å¼
        store = QdrantStore()
        results = store.search(args.search)
        print(f"\nğŸ” è¯­ä¹‰æœç´¢: {args.search}")
        print(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
        for r in results:
            print(f"  [{r['_score']:.4f}] {r['table_name']}: {r.get('table_comment', '')}")
    else:
        # å¯¼å…¥æ¨¡å¼
        import_from_json(args.file, args.delete)


if __name__ == "__main__":
    main()

"""
Elasticsearch Schema Store

å°†MySQLè¡¨ç»“æ„ä¿¡æ¯å­˜å‚¨åˆ°Elasticsearchï¼Œç”¨äºå…³é”®è¯æ£€ç´¢ã€‚
"""
import json
from typing import Any, Optional
from pathlib import Path
from elasticsearch import Elasticsearch

from config import settings


# ESç´¢å¼•åç§°
ES_INDEX_NAME = "mysql_table_schema"

# ES Mappingé…ç½®
ES_MAPPING = {
    "properties": {
        "table_name": {"type": "text", "analyzer": "ik_max_word"},
        "table_comment": {"type": "text", "analyzer": "ik_max_word"},
        "column_names_str": {"type": "text", "analyzer": "ik_max_word"},
        "column_comments_str": {"type": "text", "analyzer": "ik_max_word"},
        "full_ddl": {"type": "keyword"},  # åŸå§‹DDLï¼Œä¸åˆ†è¯
        "columns": {"type": "object", "enabled": False},  # ä¸ç´¢å¼•columnså¯¹è±¡
    }
}


class ElasticsearchStore:
    """
    Elasticsearchå­˜å‚¨ç±»ï¼Œç”¨äºç®¡ç†MySQLè¡¨ç»“æ„çš„ESç´¢å¼•ã€‚
    """
    
    def __init__(
        self,
        host: Optional[str] = None,
        port: Optional[int] = None,
        username: Optional[str] = None,
        password: Optional[str] = None,
        index_name: str = ES_INDEX_NAME,
    ):
        """
        åˆå§‹åŒ–Elasticsearchè¿æ¥ã€‚
        
        Args:
            host: ESä¸»æœºåœ°å€
            port: ESç«¯å£
            username: ESç”¨æˆ·å
            password: ESå¯†ç 
            index_name: ç´¢å¼•åç§°
        """
        self.host = host or settings.es_host
        self.port = port or settings.es_port
        self.username = (username or settings.es_user).strip()
        self.password = (password or settings.es_password).strip()
        self.index_name = index_name
        
        # æ„å»ºè¿æ¥URL
        es_url = f"http://{self.host}:{self.port}"
        
        # åˆ›å»ºESå®¢æˆ·ç«¯
        if self.username and self.password:
            self._client = Elasticsearch(
                hosts=[es_url],
                basic_auth=(self.username, self.password),
            )
        else:
            self._client = Elasticsearch(hosts=[es_url])
    
    def create_index(self, delete_existing: bool = False) -> None:
        """
        åˆ›å»ºESç´¢å¼•ã€‚
        
        Args:
            delete_existing: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„ç´¢å¼•
        """
        # åˆ é™¤å·²å­˜åœ¨çš„ç´¢å¼•
        if delete_existing:
            print(f"ğŸ—‘ï¸  åˆ é™¤å·²å­˜åœ¨çš„ç´¢å¼•: {self.index_name}")
            try:
                self._client.indices.delete(index=self.index_name)
            except Exception:
                pass  # ç´¢å¼•ä¸å­˜åœ¨æ—¶å¿½ç•¥é”™è¯¯
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        try:
            exists = self._client.indices.exists(index=self.index_name).body
        except Exception:
            # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ESå®¢æˆ·ç«¯
            try:
                self._client.indices.get(index=self.index_name)
                exists = True
            except Exception:
                exists = False
        
        if exists:
            print(f"â„¹ï¸  ç´¢å¼•å·²å­˜åœ¨: {self.index_name}")
            return
        
        # åˆ›å»ºç´¢å¼•
        print(f"ğŸ“¦ åˆ›å»ºç´¢å¼•: {self.index_name}")
        self._client.indices.create(
            index=self.index_name,
            mappings=ES_MAPPING
        )
    
    def index_schema(self, schema: dict[str, Any]) -> None:
        """
        ç´¢å¼•å•ä¸ªè¡¨çš„schemaã€‚
        
        Args:
            schema: è¡¨ç»“æ„ä¿¡æ¯å­—å…¸
        """
        doc_id = schema.get("table_name", "")
        self._client.index(
            index=self.index_name,
            id=doc_id,
            document=schema,
        )
    
    def bulk_index(self, schemas: list[dict[str, Any]]) -> int:
        """
        æ‰¹é‡ç´¢å¼•è¡¨ç»“æ„ã€‚
        
        Args:
            schemas: è¡¨ç»“æ„åˆ—è¡¨
            
        Returns:
            æˆåŠŸç´¢å¼•çš„æ•°é‡
        """
        from elasticsearch.helpers import bulk
        
        actions = [
            {
                "_index": self.index_name,
                "_id": schema.get("table_name", ""),
                "_source": schema,
            }
            for schema in schemas
        ]
        
        success, _ = bulk(self._client, actions)
        return success
    
    def search(
        self,
        query: str,
        size: int = 10,
        fields: Optional[list[str]] = None,
    ) -> list[dict[str, Any]]:
        """
        æœç´¢è¡¨ç»“æ„ã€‚
        
        Args:
            query: æœç´¢å…³é”®è¯
            size: è¿”å›ç»“æœæ•°é‡
            fields: æœç´¢å­—æ®µåˆ—è¡¨ï¼ˆæ”¯æŒæƒé‡è®¾ç½®ï¼Œå¦‚ "field^2"ï¼‰
            
        Returns:
            åŒ¹é…çš„è¡¨ç»“æ„åˆ—è¡¨
        """
        if fields is None:
            # é»˜è®¤å­—æ®µæƒé‡é…ç½®
            fields = [
                "table_name^1",           # è¡¨åæƒé‡ 1
                "column_names_str^1.5",   # åˆ—åæƒé‡ 1.5
                "table_comment^5",        # è¡¨æ³¨é‡Šæƒé‡ 5
                "column_comments_str^5"   # åˆ—æ³¨é‡Šæƒé‡ 5
            ]
        
        search_query = {
            "multi_match": {
                "query": query,
                "fields": fields,
                "type": "best_fields",
            }
        }
        
        response = self._client.search(index=self.index_name, query=search_query, size=size)
        
        results = []
        for hit in response["hits"]["hits"]:
            result = hit["_source"]
            result["_score"] = hit["_score"]
            results.append(result)
        
        return results
    
    def get_table(self, table_name: str) -> Optional[dict[str, Any]]:
        """
        è·å–æŒ‡å®šè¡¨çš„schemaã€‚
        
        Args:
            table_name: è¡¨å
            
        Returns:
            è¡¨ç»“æ„ä¿¡æ¯ï¼Œä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            response = self._client.get(index=self.index_name, id=table_name)
            return response["_source"]
        except Exception:
            return None


def import_from_json(json_file: str, delete_existing: bool = False) -> None:
    """
    ä»JSONæ–‡ä»¶å¯¼å…¥è¡¨ç»“æ„åˆ°ESã€‚
    
    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        delete_existing: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„ç´¢å¼•
    """
    with open(json_file, "r", encoding="utf-8") as f:
        schemas = json.load(f)
    
    if not isinstance(schemas, list):
        schemas = [schemas]
    
    store = ElasticsearchStore()
    store.create_index(delete_existing=delete_existing)
    
    print(f"ğŸ“¥ å¯¼å…¥ {len(schemas)} ä¸ªè¡¨ç»“æ„åˆ°ES...")
    count = store.bulk_index(schemas)
    print(f"âœ… æˆåŠŸå¯¼å…¥ {count} ä¸ªè¡¨ç»“æ„")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å°†MySQLè¡¨ç»“æ„å¯¼å…¥Elasticsearch")
    parser.add_argument(
        "-f", "--file",
        default="schema/all_tables.json",
        help="JSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: schema/all_tables.json)"
    )
    parser.add_argument(
        "-d", "--delete",
        action="store_true",
        help="åˆ é™¤å·²å­˜åœ¨çš„ç´¢å¼•åé‡æ–°åˆ›å»º"
    )
    parser.add_argument(
        "-s", "--search",
        help="æœç´¢å…³é”®è¯ï¼ˆæµ‹è¯•ç”¨ï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.search:
        # æœç´¢æ¨¡å¼
        store = ElasticsearchStore()
        results = store.search(args.search)
        print(f"\nğŸ” æœç´¢: {args.search}")
        print(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
        for r in results:
            print(f"  [{r['_score']:.2f}] {r['table_name']}: {r.get('table_comment', '')}")
    else:
        # å¯¼å…¥æ¨¡å¼
        import_from_json(args.file, args.delete)


if __name__ == "__main__":
    main()
